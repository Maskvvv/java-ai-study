spring.application.name=ollama-study

# 指定 Spring AI 使用 Ollama 作为默认对话模型实现
spring.ai.model.chat=ollama

# 本地 Ollama 服务的访问地址，需与实际运行端口保持一致
spring.ai.ollama.base-url=http://localhost:11434

# 默认使用的 Ollama 模型名称，这里固定为 qwen2.5-coder:7b
spring.ai.ollama.chat.options.model=qwen2.5-coder:7b

# 启动时不自动拉取模型，避免因网络或下载失败导致应用启动失败
spring.ai.ollama.init.pull-model-strategy=never
